{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment No. 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Name: Vivek Vitthal Avhad (4031)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-03 18:30:01.428646: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-11-03 18:30:01.536921: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-11-03 18:30:01.623820: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762174801.710997    8676 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762174801.734662    8676 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762174801.915755    8676 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762174801.915795    8676 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762174801.915799    8676 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762174801.915801    8676 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-03 18:30:01.934051: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# Conditional GAN (cGAN) on CIFAR-10\n",
        "# Grayscale -> Color Translation\n",
        "\n",
        "import os, time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers  #type:ignore\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------ Config -----------------\n",
        "BATCH_SIZE   = 256\n",
        "EPOCHS       = 10  # Reduced from 100 to 10 for faster training\n",
        "LAMBDA_L1    = 100.0\n",
        "LR           = 2e-4\n",
        "BETA_1       = 0.5\n",
        "IMG_SIZE     = 32\n",
        "CHANNELS     = 3\n",
        "SAMPLE_DIR   = \"cgan_cifar_samples\"\n",
        "os.makedirs(SAMPLE_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data\n",
        "(x_train, _), (x_test, _) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test  = x_test.astype(\"float32\") / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Condition = grayscale version, Target = color\n",
        "def rgb_to_gray(x):\n",
        "    return np.mean(x, axis=-1, keepdims=True)\n",
        "\n",
        "X_cond_train = rgb_to_gray(x_train)  # grayscale\n",
        "Y_tgt_train  = x_train               # color\n",
        "X_cond_test  = rgb_to_gray(x_test)\n",
        "Y_tgt_test   = x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale [-1,1]\n",
        "def scale_neg1_1(x): return x * 2.0 - 1.0\n",
        "X_cond_train = scale_neg1_1(X_cond_train)\n",
        "Y_tgt_train  = scale_neg1_1(Y_tgt_train)\n",
        "X_cond_test  = scale_neg1_1(X_cond_test)\n",
        "Y_tgt_test   = scale_neg1_1(Y_tgt_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-03 18:30:10.235768: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2025-11-03 18:30:10.239730: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 204800000 exceeds 10% of free system memory.\n",
            "2025-11-03 18:30:11.628099: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 614400000 exceeds 10% of free system memory.\n",
            "2025-11-03 18:30:12.526317: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 40960000 exceeds 10% of free system memory.\n",
            "2025-11-03 18:30:11.628099: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 614400000 exceeds 10% of free system memory.\n",
            "2025-11-03 18:30:12.526317: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 40960000 exceeds 10% of free system memory.\n",
            "2025-11-03 18:30:12.667774: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 122880000 exceeds 10% of free system memory.\n",
            "2025-11-03 18:30:12.667774: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 122880000 exceeds 10% of free system memory.\n"
          ]
        }
      ],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((X_cond_train, Y_tgt_train)).shuffle(60000).batch(BATCH_SIZE)\n",
        "test_ds  = tf.data.Dataset.from_tensor_slices((X_cond_test, Y_tgt_test)).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Models\n",
        "def down_block(filters, apply_bn=True):\n",
        "    block = tf.keras.Sequential()\n",
        "    block.add(layers.Conv2D(filters, 4, strides=2, padding=\"same\", use_bias=not apply_bn))\n",
        "    if apply_bn:\n",
        "        block.add(layers.BatchNormalization())\n",
        "    block.add(layers.LeakyReLU(0.2))\n",
        "    return block\n",
        "\n",
        "def up_block(filters, apply_dropout=False):\n",
        "    block = tf.keras.Sequential()\n",
        "    block.add(layers.Conv2DTranspose(filters, 4, strides=2, padding=\"same\", use_bias=False))\n",
        "    block.add(layers.BatchNormalization())\n",
        "    if apply_dropout:\n",
        "        block.add(layers.Dropout(0.5))\n",
        "    block.add(layers.ReLU())\n",
        "    return block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bigger U-Net Generator\n",
        "def build_generator():\n",
        "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 1))\n",
        "\n",
        "    d1 = down_block(64, apply_bn=False)(inputs)   # 16x16\n",
        "    d2 = down_block(128)(d1)                      # 8x8\n",
        "    d3 = down_block(256)(d2)                      # 4x4\n",
        "    d4 = down_block(512)(d3)                      # 2x2\n",
        "    d5 = down_block(512)(d4)                      # 1x1\n",
        "\n",
        "    b = layers.Conv2D(1024, 4, strides=1, padding=\"same\", use_bias=False)(d5) # 1x1\n",
        "    b = layers.BatchNormalization()(b)\n",
        "    b = layers.ReLU()(b)\n",
        "\n",
        "    # Upsampling layers - need to ensure they upsample to the correct size for concatenation and final output\n",
        "    u1 = up_block(512)(b)                          # 2x2\n",
        "    u1 = layers.Concatenate()([u1, d4])            # 2x2 + 2x2 -> 2x2\n",
        "\n",
        "    u2 = up_block(256)(u1)                         # 4x4\n",
        "    u2 = layers.Concatenate()([u2, d3])            # 4x4 + 4x4 -> 4x4\n",
        "\n",
        "    u3 = up_block(128)(u2)                         # 8x8\n",
        "    u3 = layers.Concatenate()([u3, d2])            # 8x8 + 8x8 -> 8x8\n",
        "\n",
        "    u4 = up_block(64)(u3)                          # 16x16\n",
        "    u4 = layers.Concatenate()([u4, d1])            # 16x16 + 16x16 -> 16x16\n",
        "\n",
        "    # Add an additional upsampling layer to get to 32x32\n",
        "    u5 = up_block(64)(u4)                          # 32x32\n",
        "    u5 = layers.Concatenate()([u5, inputs])        # 32x32 + 32x32 -> 32x32\n",
        "\n",
        "    out = layers.Conv2D(CHANNELS, 3, padding=\"same\", activation=\"tanh\")(u5) # 32x32x3\n",
        "    return tf.keras.Model(inputs, out, name=\"Heavy_Generator\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# PatchGAN Discriminator\n",
        "def build_discriminator():\n",
        "    inp = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 1))  # grayscale\n",
        "    tar = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))  # color\n",
        "    x = layers.Concatenate(axis=3)([inp, tar])         # 32x32x4\n",
        "\n",
        "    x = layers.Conv2D(64, 4, strides=2, padding=\"same\")(x) # 16x16\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, 4, strides=2, padding=\"same\", use_bias=False)(x) # 8x8\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(256, 4, strides=2, padding=\"same\", use_bias=False)(x) # 4x4\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(512, 4, strides=1, padding=\"same\", use_bias=False)(x) # 4x4\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    logits = layers.Conv2D(1, 4, strides=1, padding=\"same\")(x) # 4x4 Patch output\n",
        "    return tf.keras.Model([inp, tar], logits, name=\"Heavy_Discriminator\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-03 18:30:12.950091: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 16777216 exceeds 10% of free system memory.\n"
          ]
        }
      ],
      "source": [
        "G = build_generator()\n",
        "D = build_discriminator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss & Optims\n",
        "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "g_opt = tf.keras.optimizers.Adam(LR, beta_1=BETA_1)\n",
        "d_opt = tf.keras.optimizers.Adam(LR, beta_1=BETA_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def discriminator_loss(d_real, d_fake):\n",
        "    real_loss = bce(tf.ones_like(d_real), d_real)\n",
        "    fake_loss = bce(tf.zeros_like(d_fake), d_fake)\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "def generator_loss(d_fake, real, fake):\n",
        "    adv = bce(tf.ones_like(d_fake), d_fake)\n",
        "    l1 = tf.reduce_mean(tf.abs(real - fake))\n",
        "    return adv + LAMBDA_L1 * l1, adv, l1\n",
        "\n",
        "@tf.function\n",
        "def train_step(x_cond, y_tgt):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        y_fake = G(x_cond, training=True)\n",
        "\n",
        "        d_real = D([x_cond, y_tgt], training=True)\n",
        "        d_fake = D([x_cond, y_fake], training=True)\n",
        "\n",
        "        d_loss = discriminator_loss(d_real, d_fake)\n",
        "        g_total, g_adv, g_l1 = generator_loss(d_fake, y_tgt, y_fake)\n",
        "\n",
        "    d_grads = tape.gradient(d_loss, D.trainable_variables)\n",
        "    g_grads = tape.gradient(g_total, G.trainable_variables)\n",
        "\n",
        "    d_opt.apply_gradients(zip(d_grads, D.trainable_variables))\n",
        "    g_opt.apply_gradients(zip(g_grads, G.trainable_variables))\n",
        "\n",
        "    return d_loss, g_total, g_adv, g_l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization\n",
        "def show_samples(epoch):\n",
        "    x_batch, y_batch = next(iter(test_ds))\n",
        "    y_pred = G(x_batch, training=False)\n",
        "    def to01(t): return (t + 1.0) / 2.0\n",
        "    x = to01(x_batch).numpy()\n",
        "    y = to01(y_batch).numpy()\n",
        "    p = to01(y_pred).numpy()\n",
        "\n",
        "    n = 6\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for i in range(n):\n",
        "        ax = plt.subplot(3, n, i+1)\n",
        "        plt.imshow(x[i,...,0], cmap=\"gray\")\n",
        "        ax.set_title(\"Gray\")\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        ax = plt.subplot(3, n, i+1+n)\n",
        "        plt.imshow(y[i])\n",
        "        ax.set_title(\"Real Color\")\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        ax = plt.subplot(3, n, i+1+2*n)\n",
        "        plt.imshow(p[i])\n",
        "        ax.set_title(\"Fake Color\")\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.suptitle(f\"Epoch {epoch}\")\n",
        "    path = os.path.join(SAMPLE_DIR, f\"epoch_{epoch:03d}.png\")\n",
        "    plt.savefig(path)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZehBQ9X3ZnhB",
        "outputId": "af4a7258-84b8-4815-c638-12ae3841e13e"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    d_losses, g_losses = [], []\n",
        "    for x_cond, y_tgt in train_ds:\n",
        "        d_loss, g_total, g_adv, g_l1 = train_step(x_cond, y_tgt)\n",
        "        d_losses.append(d_loss.numpy())\n",
        "        g_losses.append(g_total.numpy())\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | D: {np.mean(d_losses):.4f} | G: {np.mean(g_losses):.4f} | Time: {time.time()-t0:.1f}s\")\n",
        "\n",
        "    if epoch % 2 == 0 or epoch == 1:  # Show samples every 2 epochs instead of 5\n",
        "        show_samples(epoch)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "clgenv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
